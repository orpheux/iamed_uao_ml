2025-06-05 20:45:28,014:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-06-05 20:45:28,014:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-06-05 20:45:28,014:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-06-05 20:45:28,014:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-06-05 20:45:40,409:WARNING:C:\Users\ORPHEUX\AppData\Local\Temp\ipykernel_21400\1817312141.py:816: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources

2025-06-05 20:45:48,098:INFO:PyCaret ClassificationExperiment
2025-06-05 20:45:48,098:INFO:Logging name: clf-default-name
2025-06-05 20:45:48,099:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2025-06-05 20:45:48,099:INFO:version 3.3.2
2025-06-05 20:45:48,099:INFO:Initializing setup()
2025-06-05 20:45:48,099:INFO:self.USI: 7413
2025-06-05 20:45:48,099:INFO:self._variable_keys: {'fix_imbalance', 'seed', 'X_test', 'y_test', 'fold_generator', 'pipeline', 'y', 'exp_id', 'logging_param', 'is_multiclass', 'y_train', '_available_plots', 'html_param', 'gpu_n_jobs_param', 'target_param', 'n_jobs_param', 'X', 'log_plots_param', 'memory', 'fold_groups_param', 'fold_shuffle_param', 'exp_name_log', '_ml_usecase', 'data', 'gpu_param', 'USI', 'X_train', 'idx'}
2025-06-05 20:45:48,099:INFO:Checking environment
2025-06-05 20:45:48,099:INFO:python_version: 3.10.11
2025-06-05 20:45:48,099:INFO:python_build: ('tags/v3.10.11:7d4cc5a', 'Apr  5 2023 00:38:17')
2025-06-05 20:45:48,099:INFO:machine: AMD64
2025-06-05 20:45:48,099:INFO:platform: Windows-10-10.0.26100-SP0
2025-06-05 20:45:48,108:INFO:Memory: svmem(total=25146523648, available=3963269120, percent=84.2, used=21183254528, free=3963269120)
2025-06-05 20:45:48,108:INFO:Physical Core: 8
2025-06-05 20:45:48,108:INFO:Logical Core: 16
2025-06-05 20:45:48,108:INFO:Checking libraries
2025-06-05 20:45:48,108:INFO:System:
2025-06-05 20:45:48,108:INFO:    python: 3.10.11 (tags/v3.10.11:7d4cc5a, Apr  5 2023, 00:38:17) [MSC v.1929 64 bit (AMD64)]
2025-06-05 20:45:48,109:INFO:executable: d:\Projects\clase_ia\AA_CLASE4\.venv\Scripts\python.exe
2025-06-05 20:45:48,109:INFO:   machine: Windows-10-10.0.26100-SP0
2025-06-05 20:45:48,109:INFO:PyCaret required dependencies:
2025-06-05 20:45:48,146:INFO:                 pip: 23.3.1
2025-06-05 20:45:48,146:INFO:          setuptools: 80.9.0
2025-06-05 20:45:48,146:INFO:             pycaret: 3.3.2
2025-06-05 20:45:48,146:INFO:             IPython: 8.37.0
2025-06-05 20:45:48,146:INFO:          ipywidgets: 8.1.7
2025-06-05 20:45:48,146:INFO:                tqdm: 4.67.1
2025-06-05 20:45:48,146:INFO:               numpy: 1.26.4
2025-06-05 20:45:48,146:INFO:              pandas: 2.1.4
2025-06-05 20:45:48,146:INFO:              jinja2: 3.1.6
2025-06-05 20:45:48,146:INFO:               scipy: 1.11.4
2025-06-05 20:45:48,147:INFO:              joblib: 1.3.2
2025-06-05 20:45:48,147:INFO:             sklearn: 1.4.2
2025-06-05 20:45:48,147:INFO:                pyod: 2.0.5
2025-06-05 20:45:48,147:INFO:            imblearn: 0.13.0
2025-06-05 20:45:48,147:INFO:   category_encoders: 2.7.0
2025-06-05 20:45:48,147:INFO:            lightgbm: 4.6.0
2025-06-05 20:45:48,147:INFO:               numba: 0.61.2
2025-06-05 20:45:48,147:INFO:            requests: 2.32.3
2025-06-05 20:45:48,147:INFO:          matplotlib: 3.7.5
2025-06-05 20:45:48,147:INFO:          scikitplot: 0.3.7
2025-06-05 20:45:48,147:INFO:         yellowbrick: 1.5
2025-06-05 20:45:48,147:INFO:              plotly: 5.24.1
2025-06-05 20:45:48,147:INFO:    plotly-resampler: Not installed
2025-06-05 20:45:48,147:INFO:             kaleido: 0.2.1
2025-06-05 20:45:48,147:INFO:           schemdraw: 0.15
2025-06-05 20:45:48,147:INFO:         statsmodels: 0.14.4
2025-06-05 20:45:48,147:INFO:              sktime: 0.26.0
2025-06-05 20:45:48,148:INFO:               tbats: 1.1.3
2025-06-05 20:45:48,148:INFO:            pmdarima: 2.0.4
2025-06-05 20:45:48,148:INFO:              psutil: 7.0.0
2025-06-05 20:45:48,148:INFO:          markupsafe: 3.0.2
2025-06-05 20:45:48,148:INFO:             pickle5: Not installed
2025-06-05 20:45:48,148:INFO:         cloudpickle: 3.1.1
2025-06-05 20:45:48,148:INFO:         deprecation: 2.1.0
2025-06-05 20:45:48,148:INFO:              xxhash: 3.5.0
2025-06-05 20:45:48,148:INFO:           wurlitzer: Not installed
2025-06-05 20:45:48,148:INFO:PyCaret optional dependencies:
2025-06-05 20:45:48,178:INFO:                shap: Not installed
2025-06-05 20:45:48,178:INFO:           interpret: Not installed
2025-06-05 20:45:48,178:INFO:                umap: Not installed
2025-06-05 20:45:48,178:INFO:     ydata_profiling: Not installed
2025-06-05 20:45:48,178:INFO:  explainerdashboard: Not installed
2025-06-05 20:45:48,178:INFO:             autoviz: Not installed
2025-06-05 20:45:48,178:INFO:           fairlearn: Not installed
2025-06-05 20:45:48,178:INFO:          deepchecks: Not installed
2025-06-05 20:45:48,178:INFO:             xgboost: Not installed
2025-06-05 20:45:48,178:INFO:            catboost: Not installed
2025-06-05 20:45:48,178:INFO:              kmodes: Not installed
2025-06-05 20:45:48,179:INFO:             mlxtend: Not installed
2025-06-05 20:45:48,179:INFO:       statsforecast: Not installed
2025-06-05 20:45:48,179:INFO:        tune_sklearn: Not installed
2025-06-05 20:45:48,179:INFO:                 ray: Not installed
2025-06-05 20:45:48,179:INFO:            hyperopt: Not installed
2025-06-05 20:45:48,179:INFO:              optuna: Not installed
2025-06-05 20:45:48,179:INFO:               skopt: Not installed
2025-06-05 20:45:48,179:INFO:              mlflow: Not installed
2025-06-05 20:45:48,179:INFO:              gradio: Not installed
2025-06-05 20:45:48,179:INFO:             fastapi: Not installed
2025-06-05 20:45:48,179:INFO:             uvicorn: Not installed
2025-06-05 20:45:48,179:INFO:              m2cgen: Not installed
2025-06-05 20:45:48,179:INFO:           evidently: Not installed
2025-06-05 20:45:48,179:INFO:               fugue: Not installed
2025-06-05 20:45:48,179:INFO:           streamlit: Not installed
2025-06-05 20:45:48,179:INFO:             prophet: Not installed
2025-06-05 20:45:48,179:INFO:None
2025-06-05 20:45:48,179:INFO:Set up data.
2025-06-05 20:45:48,184:INFO:Set up folding strategy.
2025-06-05 20:45:48,184:INFO:Set up train/test split.
2025-06-05 20:45:48,187:INFO:Set up index.
2025-06-05 20:45:48,187:INFO:Assigning column types.
2025-06-05 20:45:48,188:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2025-06-05 20:45:48,235:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-06-05 20:45:48,235:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-06-05 20:45:48,268:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-06-05 20:45:48,268:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-06-05 20:45:48,317:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-06-05 20:45:48,318:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-06-05 20:45:48,342:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-06-05 20:45:48,342:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-06-05 20:45:48,342:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2025-06-05 20:45:48,390:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-06-05 20:45:48,418:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-06-05 20:45:48,419:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-06-05 20:45:48,464:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-06-05 20:45:48,491:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-06-05 20:45:48,491:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-06-05 20:45:48,491:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2025-06-05 20:45:48,566:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-06-05 20:45:48,566:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-06-05 20:45:48,635:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-06-05 20:45:48,635:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-06-05 20:45:48,642:INFO:Preparing preprocessing pipeline...
2025-06-05 20:45:48,642:INFO:Set up simple imputation.
2025-06-05 20:45:48,642:INFO:Set up column name cleaning.
2025-06-05 20:45:48,668:INFO:Finished creating preprocessing pipeline.
2025-06-05 20:45:48,672:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\ORPHEUX\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['sepal length (cm)',
                                             'sepal width (cm)',
                                             'petal length (cm)',
                                             'petal width (cm)'],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='mea...
                ('categorical_imputer',
                 TransformerWrapper(exclude=None, include=[],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='most_frequent'))),
                ('clean_column_names',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=CleanColumnNames(match='[\\]\\[\\,\\{\\}\\"\\:]+')))],
         verbose=False)
2025-06-05 20:45:48,672:INFO:Creating final display dataframe.
2025-06-05 20:45:48,728:INFO:Setup _display_container:                     Description             Value
0                    Session id               123
1                        Target            target
2                   Target type        Multiclass
3           Original data shape          (150, 5)
4        Transformed data shape          (150, 5)
5   Transformed train set shape          (105, 5)
6    Transformed test set shape           (45, 5)
7              Numeric features                 4
8                    Preprocess              True
9               Imputation type            simple
10           Numeric imputation              mean
11       Categorical imputation              mode
12               Fold Generator   StratifiedKFold
13                  Fold Number                10
14                     CPU Jobs                -1
15                      Use GPU             False
16               Log Experiment             False
17              Experiment Name  clf-default-name
18                          USI              7413
2025-06-05 20:45:48,809:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-06-05 20:45:48,809:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-06-05 20:45:48,890:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-06-05 20:45:48,890:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-06-05 20:45:48,891:INFO:setup() successfully completed in 0.79s...............
2025-06-05 20:47:28,627:INFO:Initializing compare_models()
2025-06-05 20:47:28,627:INFO:compare_models(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000015F18134700>, include=None, fold=None, round=4, cross_validation=True, sort=Accuracy, n_select=5, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.classification.oop.ClassificationExperiment object at 0x0000015F18134700>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'Accuracy', 'n_select': 5, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'probability_threshold': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.classification.oop.ClassificationExperiment'>}, exclude=None)
2025-06-05 20:47:28,627:INFO:Checking exceptions
2025-06-05 20:47:28,631:INFO:Preparing display monitor
2025-06-05 20:47:28,676:INFO:Initializing Logistic Regression
2025-06-05 20:47:28,676:INFO:Total runtime is 0.0 minutes
2025-06-05 20:47:28,685:INFO:SubProcess create_model() called ==================================
2025-06-05 20:47:28,685:INFO:Initializing create_model()
2025-06-05 20:47:28,685:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000015F18134700>, estimator=lr, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000015F1800E320>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-06-05 20:47:28,685:INFO:Checking exceptions
2025-06-05 20:47:28,686:INFO:Importing libraries
2025-06-05 20:47:28,686:INFO:Copying training dataset
2025-06-05 20:47:28,690:INFO:Defining folds
2025-06-05 20:47:28,690:INFO:Declaring metric variables
2025-06-05 20:47:28,699:INFO:Importing untrained model
2025-06-05 20:47:28,709:INFO:Logistic Regression Imported successfully
2025-06-05 20:47:28,730:INFO:Starting cross validation
2025-06-05 20:47:28,731:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-06-05 20:47:33,870:WARNING:d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-05 20:47:34,019:WARNING:d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-05 20:47:34,141:WARNING:d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-05 20:47:34,154:WARNING:d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-05 20:47:34,176:WARNING:d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-05 20:47:34,232:WARNING:d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-05 20:47:34,234:WARNING:d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-05 20:47:34,245:WARNING:d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-05 20:47:34,261:WARNING:d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-05 20:47:34,282:WARNING:d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-05 20:47:34,314:INFO:Calculating mean and std
2025-06-05 20:47:34,316:INFO:Creating metrics dataframe
2025-06-05 20:47:34,319:INFO:Uploading results into container
2025-06-05 20:47:34,320:INFO:Uploading model into container now
2025-06-05 20:47:34,321:INFO:_master_model_container: 1
2025-06-05 20:47:34,321:INFO:_display_container: 2
2025-06-05 20:47:34,321:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=123, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2025-06-05 20:47:34,321:INFO:create_model() successfully completed......................................
2025-06-05 20:47:34,405:INFO:SubProcess create_model() end ==================================
2025-06-05 20:47:34,406:INFO:Creating metrics dataframe
2025-06-05 20:47:34,413:INFO:Initializing K Neighbors Classifier
2025-06-05 20:47:34,414:INFO:Total runtime is 0.09563928445180257 minutes
2025-06-05 20:47:34,420:INFO:SubProcess create_model() called ==================================
2025-06-05 20:47:34,420:INFO:Initializing create_model()
2025-06-05 20:47:34,420:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000015F18134700>, estimator=knn, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000015F1800E320>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-06-05 20:47:34,420:INFO:Checking exceptions
2025-06-05 20:47:34,420:INFO:Importing libraries
2025-06-05 20:47:34,420:INFO:Copying training dataset
2025-06-05 20:47:34,425:INFO:Defining folds
2025-06-05 20:47:34,425:INFO:Declaring metric variables
2025-06-05 20:47:34,433:INFO:Importing untrained model
2025-06-05 20:47:34,441:INFO:K Neighbors Classifier Imported successfully
2025-06-05 20:47:34,458:INFO:Starting cross validation
2025-06-05 20:47:34,459:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-06-05 20:47:38,078:INFO:Calculating mean and std
2025-06-05 20:47:38,081:INFO:Creating metrics dataframe
2025-06-05 20:47:38,084:INFO:Uploading results into container
2025-06-05 20:47:38,085:INFO:Uploading model into container now
2025-06-05 20:47:38,086:INFO:_master_model_container: 2
2025-06-05 20:47:38,087:INFO:_display_container: 2
2025-06-05 20:47:38,087:INFO:KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
                     metric_params=None, n_jobs=-1, n_neighbors=5, p=2,
                     weights='uniform')
2025-06-05 20:47:38,087:INFO:create_model() successfully completed......................................
2025-06-05 20:47:38,164:INFO:SubProcess create_model() end ==================================
2025-06-05 20:47:38,164:INFO:Creating metrics dataframe
2025-06-05 20:47:38,188:INFO:Initializing Naive Bayes
2025-06-05 20:47:38,188:INFO:Total runtime is 0.15853427251180013 minutes
2025-06-05 20:47:38,193:INFO:SubProcess create_model() called ==================================
2025-06-05 20:47:38,193:INFO:Initializing create_model()
2025-06-05 20:47:38,193:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000015F18134700>, estimator=nb, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000015F1800E320>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-06-05 20:47:38,194:INFO:Checking exceptions
2025-06-05 20:47:38,194:INFO:Importing libraries
2025-06-05 20:47:38,194:INFO:Copying training dataset
2025-06-05 20:47:38,198:INFO:Defining folds
2025-06-05 20:47:38,198:INFO:Declaring metric variables
2025-06-05 20:47:38,205:INFO:Importing untrained model
2025-06-05 20:47:38,216:INFO:Naive Bayes Imported successfully
2025-06-05 20:47:38,231:INFO:Starting cross validation
2025-06-05 20:47:38,232:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-06-05 20:47:38,313:INFO:Calculating mean and std
2025-06-05 20:47:38,316:INFO:Creating metrics dataframe
2025-06-05 20:47:38,318:INFO:Uploading results into container
2025-06-05 20:47:38,319:INFO:Uploading model into container now
2025-06-05 20:47:38,319:INFO:_master_model_container: 3
2025-06-05 20:47:38,320:INFO:_display_container: 2
2025-06-05 20:47:38,320:INFO:GaussianNB(priors=None, var_smoothing=1e-09)
2025-06-05 20:47:38,320:INFO:create_model() successfully completed......................................
2025-06-05 20:47:38,406:INFO:SubProcess create_model() end ==================================
2025-06-05 20:47:38,406:INFO:Creating metrics dataframe
2025-06-05 20:47:38,415:INFO:Initializing Decision Tree Classifier
2025-06-05 20:47:38,416:INFO:Total runtime is 0.16234139998753866 minutes
2025-06-05 20:47:38,424:INFO:SubProcess create_model() called ==================================
2025-06-05 20:47:38,424:INFO:Initializing create_model()
2025-06-05 20:47:38,425:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000015F18134700>, estimator=dt, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000015F1800E320>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-06-05 20:47:38,425:INFO:Checking exceptions
2025-06-05 20:47:38,425:INFO:Importing libraries
2025-06-05 20:47:38,425:INFO:Copying training dataset
2025-06-05 20:47:38,430:INFO:Defining folds
2025-06-05 20:47:38,430:INFO:Declaring metric variables
2025-06-05 20:47:38,437:INFO:Importing untrained model
2025-06-05 20:47:38,444:INFO:Decision Tree Classifier Imported successfully
2025-06-05 20:47:38,457:INFO:Starting cross validation
2025-06-05 20:47:38,458:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-06-05 20:47:38,576:INFO:Calculating mean and std
2025-06-05 20:47:38,579:INFO:Creating metrics dataframe
2025-06-05 20:47:38,582:INFO:Uploading results into container
2025-06-05 20:47:38,583:INFO:Uploading model into container now
2025-06-05 20:47:38,584:INFO:_master_model_container: 4
2025-06-05 20:47:38,584:INFO:_display_container: 2
2025-06-05 20:47:38,585:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=123, splitter='best')
2025-06-05 20:47:38,585:INFO:create_model() successfully completed......................................
2025-06-05 20:47:38,689:INFO:SubProcess create_model() end ==================================
2025-06-05 20:47:38,690:INFO:Creating metrics dataframe
2025-06-05 20:47:38,698:INFO:Initializing SVM - Linear Kernel
2025-06-05 20:47:38,698:INFO:Total runtime is 0.16703578233718872 minutes
2025-06-05 20:47:38,698:INFO:SubProcess create_model() called ==================================
2025-06-05 20:47:38,698:INFO:Initializing create_model()
2025-06-05 20:47:38,698:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000015F18134700>, estimator=svm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000015F1800E320>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-06-05 20:47:38,698:INFO:Checking exceptions
2025-06-05 20:47:38,698:INFO:Importing libraries
2025-06-05 20:47:38,698:INFO:Copying training dataset
2025-06-05 20:47:38,712:INFO:Defining folds
2025-06-05 20:47:38,714:INFO:Declaring metric variables
2025-06-05 20:47:38,723:INFO:Importing untrained model
2025-06-05 20:47:38,731:INFO:SVM - Linear Kernel Imported successfully
2025-06-05 20:47:38,746:INFO:Starting cross validation
2025-06-05 20:47:38,748:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-06-05 20:47:38,825:WARNING:d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-05 20:47:38,826:WARNING:d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-05 20:47:38,828:WARNING:d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-05 20:47:38,832:WARNING:d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-05 20:47:38,837:WARNING:d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-06-05 20:47:38,842:WARNING:d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-06-05 20:47:38,846:WARNING:d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-05 20:47:38,852:WARNING:d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-05 20:47:38,852:WARNING:d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-05 20:47:38,854:WARNING:d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-05 20:47:38,855:WARNING:d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-05 20:47:38,856:WARNING:d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-05 20:47:38,861:WARNING:d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-06-05 20:47:38,861:WARNING:d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-06-05 20:47:38,861:WARNING:d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-06-05 20:47:38,863:WARNING:d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-06-05 20:47:38,880:INFO:Calculating mean and std
2025-06-05 20:47:38,881:INFO:Creating metrics dataframe
2025-06-05 20:47:38,884:INFO:Uploading results into container
2025-06-05 20:47:38,885:INFO:Uploading model into container now
2025-06-05 20:47:38,885:INFO:_master_model_container: 5
2025-06-05 20:47:38,885:INFO:_display_container: 2
2025-06-05 20:47:38,886:INFO:SGDClassifier(alpha=0.0001, average=False, class_weight=None,
              early_stopping=False, epsilon=0.1, eta0=0.001, fit_intercept=True,
              l1_ratio=0.15, learning_rate='optimal', loss='hinge',
              max_iter=1000, n_iter_no_change=5, n_jobs=-1, penalty='l2',
              power_t=0.5, random_state=123, shuffle=True, tol=0.001,
              validation_fraction=0.1, verbose=0, warm_start=False)
2025-06-05 20:47:38,886:INFO:create_model() successfully completed......................................
2025-06-05 20:47:38,963:INFO:SubProcess create_model() end ==================================
2025-06-05 20:47:38,963:INFO:Creating metrics dataframe
2025-06-05 20:47:38,973:INFO:Initializing Ridge Classifier
2025-06-05 20:47:38,973:INFO:Total runtime is 0.17162033716837566 minutes
2025-06-05 20:47:38,979:INFO:SubProcess create_model() called ==================================
2025-06-05 20:47:38,979:INFO:Initializing create_model()
2025-06-05 20:47:38,980:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000015F18134700>, estimator=ridge, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000015F1800E320>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-06-05 20:47:38,980:INFO:Checking exceptions
2025-06-05 20:47:38,980:INFO:Importing libraries
2025-06-05 20:47:38,980:INFO:Copying training dataset
2025-06-05 20:47:38,984:INFO:Defining folds
2025-06-05 20:47:38,984:INFO:Declaring metric variables
2025-06-05 20:47:38,990:INFO:Importing untrained model
2025-06-05 20:47:38,997:INFO:Ridge Classifier Imported successfully
2025-06-05 20:47:39,008:INFO:Starting cross validation
2025-06-05 20:47:39,009:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-06-05 20:47:39,051:WARNING:d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-05 20:47:39,052:WARNING:d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-05 20:47:39,055:WARNING:d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-05 20:47:39,056:WARNING:d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-05 20:47:39,058:WARNING:d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-05 20:47:39,059:WARNING:d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-05 20:47:39,061:WARNING:d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-05 20:47:39,063:WARNING:d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-05 20:47:39,063:WARNING:d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-05 20:47:39,078:INFO:Calculating mean and std
2025-06-05 20:47:39,079:INFO:Creating metrics dataframe
2025-06-05 20:47:39,081:INFO:Uploading results into container
2025-06-05 20:47:39,082:INFO:Uploading model into container now
2025-06-05 20:47:39,082:INFO:_master_model_container: 6
2025-06-05 20:47:39,083:INFO:_display_container: 2
2025-06-05 20:47:39,083:INFO:RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=123, solver='auto',
                tol=0.0001)
2025-06-05 20:47:39,083:INFO:create_model() successfully completed......................................
2025-06-05 20:47:39,159:INFO:SubProcess create_model() end ==================================
2025-06-05 20:47:39,159:INFO:Creating metrics dataframe
2025-06-05 20:47:39,167:INFO:Initializing Random Forest Classifier
2025-06-05 20:47:39,167:INFO:Total runtime is 0.17486262718836468 minutes
2025-06-05 20:47:39,172:INFO:SubProcess create_model() called ==================================
2025-06-05 20:47:39,173:INFO:Initializing create_model()
2025-06-05 20:47:39,173:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000015F18134700>, estimator=rf, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000015F1800E320>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-06-05 20:47:39,173:INFO:Checking exceptions
2025-06-05 20:47:39,173:INFO:Importing libraries
2025-06-05 20:47:39,173:INFO:Copying training dataset
2025-06-05 20:47:39,176:INFO:Defining folds
2025-06-05 20:47:39,177:INFO:Declaring metric variables
2025-06-05 20:47:39,185:INFO:Importing untrained model
2025-06-05 20:47:39,191:INFO:Random Forest Classifier Imported successfully
2025-06-05 20:47:39,205:INFO:Starting cross validation
2025-06-05 20:47:39,206:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-06-05 20:47:39,663:INFO:Calculating mean and std
2025-06-05 20:47:39,665:INFO:Creating metrics dataframe
2025-06-05 20:47:39,667:INFO:Uploading results into container
2025-06-05 20:47:39,667:INFO:Uploading model into container now
2025-06-05 20:47:39,668:INFO:_master_model_container: 7
2025-06-05 20:47:39,668:INFO:_display_container: 2
2025-06-05 20:47:39,668:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=123, verbose=0,
                       warm_start=False)
2025-06-05 20:47:39,669:INFO:create_model() successfully completed......................................
2025-06-05 20:47:39,735:INFO:SubProcess create_model() end ==================================
2025-06-05 20:47:39,735:INFO:Creating metrics dataframe
2025-06-05 20:47:39,742:INFO:Initializing Quadratic Discriminant Analysis
2025-06-05 20:47:39,742:INFO:Total runtime is 0.18444074789683026 minutes
2025-06-05 20:47:39,749:INFO:SubProcess create_model() called ==================================
2025-06-05 20:47:39,750:INFO:Initializing create_model()
2025-06-05 20:47:39,750:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000015F18134700>, estimator=qda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000015F1800E320>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-06-05 20:47:39,750:INFO:Checking exceptions
2025-06-05 20:47:39,750:INFO:Importing libraries
2025-06-05 20:47:39,750:INFO:Copying training dataset
2025-06-05 20:47:39,753:INFO:Defining folds
2025-06-05 20:47:39,753:INFO:Declaring metric variables
2025-06-05 20:47:39,759:INFO:Importing untrained model
2025-06-05 20:47:39,764:INFO:Quadratic Discriminant Analysis Imported successfully
2025-06-05 20:47:39,781:INFO:Starting cross validation
2025-06-05 20:47:39,782:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-06-05 20:47:39,818:WARNING:d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-05 20:47:39,823:WARNING:d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-05 20:47:39,823:WARNING:d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-05 20:47:39,830:WARNING:d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-05 20:47:39,830:WARNING:d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-05 20:47:39,833:WARNING:d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-05 20:47:39,834:WARNING:d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-05 20:47:39,834:WARNING:d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-05 20:47:39,835:WARNING:d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-05 20:47:39,847:INFO:Calculating mean and std
2025-06-05 20:47:39,848:INFO:Creating metrics dataframe
2025-06-05 20:47:39,850:INFO:Uploading results into container
2025-06-05 20:47:39,850:INFO:Uploading model into container now
2025-06-05 20:47:39,851:INFO:_master_model_container: 8
2025-06-05 20:47:39,851:INFO:_display_container: 2
2025-06-05 20:47:39,851:INFO:QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0,
                              store_covariance=False, tol=0.0001)
2025-06-05 20:47:39,851:INFO:create_model() successfully completed......................................
2025-06-05 20:47:39,931:INFO:SubProcess create_model() end ==================================
2025-06-05 20:47:39,932:INFO:Creating metrics dataframe
2025-06-05 20:47:39,940:INFO:Initializing Ada Boost Classifier
2025-06-05 20:47:39,940:INFO:Total runtime is 0.1877441763877869 minutes
2025-06-05 20:47:39,948:INFO:SubProcess create_model() called ==================================
2025-06-05 20:47:39,949:INFO:Initializing create_model()
2025-06-05 20:47:39,949:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000015F18134700>, estimator=ada, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000015F1800E320>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-06-05 20:47:39,949:INFO:Checking exceptions
2025-06-05 20:47:39,949:INFO:Importing libraries
2025-06-05 20:47:39,949:INFO:Copying training dataset
2025-06-05 20:47:39,953:INFO:Defining folds
2025-06-05 20:47:39,954:INFO:Declaring metric variables
2025-06-05 20:47:39,960:INFO:Importing untrained model
2025-06-05 20:47:39,970:INFO:Ada Boost Classifier Imported successfully
2025-06-05 20:47:39,985:INFO:Starting cross validation
2025-06-05 20:47:39,985:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-06-05 20:47:40,012:WARNING:d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-06-05 20:47:40,016:WARNING:d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-06-05 20:47:40,018:WARNING:d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-06-05 20:47:40,018:WARNING:d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-06-05 20:47:40,021:WARNING:d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-06-05 20:47:40,023:WARNING:d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-06-05 20:47:40,025:WARNING:d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-06-05 20:47:40,027:WARNING:d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-06-05 20:47:40,028:WARNING:d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-06-05 20:47:40,033:WARNING:d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-06-05 20:47:40,211:WARNING:d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-05 20:47:40,215:WARNING:d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-05 20:47:40,224:WARNING:d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-05 20:47:40,231:WARNING:d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-05 20:47:40,231:WARNING:d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-05 20:47:40,240:WARNING:d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-05 20:47:40,246:WARNING:d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-05 20:47:40,259:WARNING:d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-05 20:47:40,275:WARNING:d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-05 20:47:40,280:WARNING:d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-05 20:47:40,291:INFO:Calculating mean and std
2025-06-05 20:47:40,292:INFO:Creating metrics dataframe
2025-06-05 20:47:40,295:INFO:Uploading results into container
2025-06-05 20:47:40,295:INFO:Uploading model into container now
2025-06-05 20:47:40,296:INFO:_master_model_container: 9
2025-06-05 20:47:40,297:INFO:_display_container: 2
2025-06-05 20:47:40,297:INFO:AdaBoostClassifier(algorithm='SAMME.R', estimator=None, learning_rate=1.0,
                   n_estimators=50, random_state=123)
2025-06-05 20:47:40,298:INFO:create_model() successfully completed......................................
2025-06-05 20:47:40,373:INFO:SubProcess create_model() end ==================================
2025-06-05 20:47:40,373:INFO:Creating metrics dataframe
2025-06-05 20:47:40,382:INFO:Initializing Gradient Boosting Classifier
2025-06-05 20:47:40,382:INFO:Total runtime is 0.19510673284530644 minutes
2025-06-05 20:47:40,387:INFO:SubProcess create_model() called ==================================
2025-06-05 20:47:40,388:INFO:Initializing create_model()
2025-06-05 20:47:40,388:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000015F18134700>, estimator=gbc, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000015F1800E320>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-06-05 20:47:40,388:INFO:Checking exceptions
2025-06-05 20:47:40,389:INFO:Importing libraries
2025-06-05 20:47:40,389:INFO:Copying training dataset
2025-06-05 20:47:40,391:INFO:Defining folds
2025-06-05 20:47:40,391:INFO:Declaring metric variables
2025-06-05 20:47:40,398:INFO:Importing untrained model
2025-06-05 20:47:40,406:INFO:Gradient Boosting Classifier Imported successfully
2025-06-05 20:47:40,416:INFO:Starting cross validation
2025-06-05 20:47:40,416:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-06-05 20:47:40,873:WARNING:d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-05 20:47:40,892:WARNING:d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-05 20:47:40,900:WARNING:d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-05 20:47:40,907:WARNING:d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-05 20:47:40,909:WARNING:d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-05 20:47:40,909:WARNING:d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-05 20:47:40,911:WARNING:d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-05 20:47:40,915:WARNING:d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-05 20:47:40,919:WARNING:d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-05 20:47:40,933:INFO:Calculating mean and std
2025-06-05 20:47:40,933:INFO:Creating metrics dataframe
2025-06-05 20:47:40,934:INFO:Uploading results into container
2025-06-05 20:47:40,934:INFO:Uploading model into container now
2025-06-05 20:47:40,934:INFO:_master_model_container: 10
2025-06-05 20:47:40,934:INFO:_display_container: 2
2025-06-05 20:47:40,934:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=123, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2025-06-05 20:47:40,934:INFO:create_model() successfully completed......................................
2025-06-05 20:47:41,014:INFO:SubProcess create_model() end ==================================
2025-06-05 20:47:41,014:INFO:Creating metrics dataframe
2025-06-05 20:47:41,023:INFO:Initializing Linear Discriminant Analysis
2025-06-05 20:47:41,024:INFO:Total runtime is 0.20580744743347174 minutes
2025-06-05 20:47:41,030:INFO:SubProcess create_model() called ==================================
2025-06-05 20:47:41,030:INFO:Initializing create_model()
2025-06-05 20:47:41,031:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000015F18134700>, estimator=lda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000015F1800E320>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-06-05 20:47:41,031:INFO:Checking exceptions
2025-06-05 20:47:41,032:INFO:Importing libraries
2025-06-05 20:47:41,032:INFO:Copying training dataset
2025-06-05 20:47:41,032:INFO:Defining folds
2025-06-05 20:47:41,032:INFO:Declaring metric variables
2025-06-05 20:47:41,032:INFO:Importing untrained model
2025-06-05 20:47:41,049:INFO:Linear Discriminant Analysis Imported successfully
2025-06-05 20:47:41,067:INFO:Starting cross validation
2025-06-05 20:47:41,067:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-06-05 20:47:41,108:WARNING:d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-05 20:47:41,108:WARNING:d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-05 20:47:41,108:WARNING:d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-05 20:47:41,108:WARNING:d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-05 20:47:41,115:WARNING:d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-05 20:47:41,115:WARNING:d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-05 20:47:41,116:WARNING:d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-05 20:47:41,116:WARNING:d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-05 20:47:41,116:WARNING:d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-05 20:47:41,127:WARNING:d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-05 20:47:41,147:INFO:Calculating mean and std
2025-06-05 20:47:41,148:INFO:Creating metrics dataframe
2025-06-05 20:47:41,150:INFO:Uploading results into container
2025-06-05 20:47:41,150:INFO:Uploading model into container now
2025-06-05 20:47:41,150:INFO:_master_model_container: 11
2025-06-05 20:47:41,150:INFO:_display_container: 2
2025-06-05 20:47:41,150:INFO:LinearDiscriminantAnalysis(covariance_estimator=None, n_components=None,
                           priors=None, shrinkage=None, solver='svd',
                           store_covariance=False, tol=0.0001)
2025-06-05 20:47:41,150:INFO:create_model() successfully completed......................................
2025-06-05 20:47:41,230:INFO:SubProcess create_model() end ==================================
2025-06-05 20:47:41,230:INFO:Creating metrics dataframe
2025-06-05 20:47:41,244:INFO:Initializing Extra Trees Classifier
2025-06-05 20:47:41,244:INFO:Total runtime is 0.20948178768157966 minutes
2025-06-05 20:47:41,255:INFO:SubProcess create_model() called ==================================
2025-06-05 20:47:41,255:INFO:Initializing create_model()
2025-06-05 20:47:41,255:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000015F18134700>, estimator=et, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000015F1800E320>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-06-05 20:47:41,255:INFO:Checking exceptions
2025-06-05 20:47:41,255:INFO:Importing libraries
2025-06-05 20:47:41,255:INFO:Copying training dataset
2025-06-05 20:47:41,260:INFO:Defining folds
2025-06-05 20:47:41,260:INFO:Declaring metric variables
2025-06-05 20:47:41,265:INFO:Importing untrained model
2025-06-05 20:47:41,265:INFO:Extra Trees Classifier Imported successfully
2025-06-05 20:47:41,282:INFO:Starting cross validation
2025-06-05 20:47:41,282:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-06-05 20:47:41,747:INFO:Calculating mean and std
2025-06-05 20:47:41,748:INFO:Creating metrics dataframe
2025-06-05 20:47:41,751:INFO:Uploading results into container
2025-06-05 20:47:41,752:INFO:Uploading model into container now
2025-06-05 20:47:41,753:INFO:_master_model_container: 12
2025-06-05 20:47:41,753:INFO:_display_container: 2
2025-06-05 20:47:41,754:INFO:ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     monotonic_cst=None, n_estimators=100, n_jobs=-1,
                     oob_score=False, random_state=123, verbose=0,
                     warm_start=False)
2025-06-05 20:47:41,754:INFO:create_model() successfully completed......................................
2025-06-05 20:47:41,840:INFO:SubProcess create_model() end ==================================
2025-06-05 20:47:41,840:INFO:Creating metrics dataframe
2025-06-05 20:47:41,850:INFO:Initializing Light Gradient Boosting Machine
2025-06-05 20:47:41,850:INFO:Total runtime is 0.2195699294408163 minutes
2025-06-05 20:47:41,855:INFO:SubProcess create_model() called ==================================
2025-06-05 20:47:41,856:INFO:Initializing create_model()
2025-06-05 20:47:41,856:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000015F18134700>, estimator=lightgbm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000015F1800E320>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-06-05 20:47:41,856:INFO:Checking exceptions
2025-06-05 20:47:41,856:INFO:Importing libraries
2025-06-05 20:47:41,856:INFO:Copying training dataset
2025-06-05 20:47:41,859:INFO:Defining folds
2025-06-05 20:47:41,859:INFO:Declaring metric variables
2025-06-05 20:47:41,864:INFO:Importing untrained model
2025-06-05 20:47:41,872:INFO:Light Gradient Boosting Machine Imported successfully
2025-06-05 20:47:41,888:INFO:Starting cross validation
2025-06-05 20:47:41,889:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-06-05 20:47:42,497:INFO:Calculating mean and std
2025-06-05 20:47:42,497:INFO:Creating metrics dataframe
2025-06-05 20:47:42,502:INFO:Uploading results into container
2025-06-05 20:47:42,503:INFO:Uploading model into container now
2025-06-05 20:47:42,504:INFO:_master_model_container: 13
2025-06-05 20:47:42,504:INFO:_display_container: 2
2025-06-05 20:47:42,506:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2025-06-05 20:47:42,506:INFO:create_model() successfully completed......................................
2025-06-05 20:47:42,620:INFO:SubProcess create_model() end ==================================
2025-06-05 20:47:42,620:INFO:Creating metrics dataframe
2025-06-05 20:47:42,630:INFO:Initializing Dummy Classifier
2025-06-05 20:47:42,630:INFO:Total runtime is 0.2325791438420614 minutes
2025-06-05 20:47:42,638:INFO:SubProcess create_model() called ==================================
2025-06-05 20:47:42,638:INFO:Initializing create_model()
2025-06-05 20:47:42,639:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000015F18134700>, estimator=dummy, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000015F1800E320>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-06-05 20:47:42,639:INFO:Checking exceptions
2025-06-05 20:47:42,639:INFO:Importing libraries
2025-06-05 20:47:42,639:INFO:Copying training dataset
2025-06-05 20:47:42,643:INFO:Defining folds
2025-06-05 20:47:42,643:INFO:Declaring metric variables
2025-06-05 20:47:42,649:INFO:Importing untrained model
2025-06-05 20:47:42,656:INFO:Dummy Classifier Imported successfully
2025-06-05 20:47:42,672:INFO:Starting cross validation
2025-06-05 20:47:42,673:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-06-05 20:47:42,720:WARNING:d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-06-05 20:47:42,741:WARNING:d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-06-05 20:47:42,742:WARNING:d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-06-05 20:47:42,747:WARNING:d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-06-05 20:47:42,751:WARNING:d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-06-05 20:47:42,760:WARNING:d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-06-05 20:47:42,763:WARNING:d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-06-05 20:47:42,765:WARNING:d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-06-05 20:47:42,770:WARNING:d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-06-05 20:47:42,778:INFO:Calculating mean and std
2025-06-05 20:47:42,779:INFO:Creating metrics dataframe
2025-06-05 20:47:42,783:INFO:Uploading results into container
2025-06-05 20:47:42,784:INFO:Uploading model into container now
2025-06-05 20:47:42,785:INFO:_master_model_container: 14
2025-06-05 20:47:42,785:INFO:_display_container: 2
2025-06-05 20:47:42,785:INFO:DummyClassifier(constant=None, random_state=123, strategy='prior')
2025-06-05 20:47:42,785:INFO:create_model() successfully completed......................................
2025-06-05 20:47:42,875:INFO:SubProcess create_model() end ==================================
2025-06-05 20:47:42,875:INFO:Creating metrics dataframe
2025-06-05 20:47:42,889:WARNING:d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:323: FutureWarning: Styler.applymap has been deprecated. Use Styler.map instead.
  master_display_.apply(

2025-06-05 20:47:42,905:INFO:Initializing create_model()
2025-06-05 20:47:42,905:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000015F18134700>, estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=123, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-06-05 20:47:42,906:INFO:Checking exceptions
2025-06-05 20:47:42,908:INFO:Importing libraries
2025-06-05 20:47:42,908:INFO:Copying training dataset
2025-06-05 20:47:42,911:INFO:Defining folds
2025-06-05 20:47:42,911:INFO:Declaring metric variables
2025-06-05 20:47:42,911:INFO:Importing untrained model
2025-06-05 20:47:42,911:INFO:Declaring custom model
2025-06-05 20:47:42,912:INFO:Logistic Regression Imported successfully
2025-06-05 20:47:42,914:INFO:Cross validation set to False
2025-06-05 20:47:42,914:INFO:Fitting Model
2025-06-05 20:47:42,935:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=123, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2025-06-05 20:47:42,935:INFO:create_model() successfully completed......................................
2025-06-05 20:47:43,030:INFO:Initializing create_model()
2025-06-05 20:47:43,030:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000015F18134700>, estimator=KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
                     metric_params=None, n_jobs=-1, n_neighbors=5, p=2,
                     weights='uniform'), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-06-05 20:47:43,030:INFO:Checking exceptions
2025-06-05 20:47:43,032:INFO:Importing libraries
2025-06-05 20:47:43,032:INFO:Copying training dataset
2025-06-05 20:47:43,035:INFO:Defining folds
2025-06-05 20:47:43,035:INFO:Declaring metric variables
2025-06-05 20:47:43,035:INFO:Importing untrained model
2025-06-05 20:47:43,035:INFO:Declaring custom model
2025-06-05 20:47:43,036:INFO:K Neighbors Classifier Imported successfully
2025-06-05 20:47:43,036:INFO:Cross validation set to False
2025-06-05 20:47:43,036:INFO:Fitting Model
2025-06-05 20:47:43,045:INFO:KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
                     metric_params=None, n_jobs=-1, n_neighbors=5, p=2,
                     weights='uniform')
2025-06-05 20:47:43,045:INFO:create_model() successfully completed......................................
2025-06-05 20:47:43,136:INFO:Initializing create_model()
2025-06-05 20:47:43,136:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000015F18134700>, estimator=QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0,
                              store_covariance=False, tol=0.0001), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-06-05 20:47:43,136:INFO:Checking exceptions
2025-06-05 20:47:43,136:INFO:Importing libraries
2025-06-05 20:47:43,136:INFO:Copying training dataset
2025-06-05 20:47:43,136:INFO:Defining folds
2025-06-05 20:47:43,136:INFO:Declaring metric variables
2025-06-05 20:47:43,136:INFO:Importing untrained model
2025-06-05 20:47:43,136:INFO:Declaring custom model
2025-06-05 20:47:43,136:INFO:Quadratic Discriminant Analysis Imported successfully
2025-06-05 20:47:43,136:INFO:Cross validation set to False
2025-06-05 20:47:43,136:INFO:Fitting Model
2025-06-05 20:47:43,151:INFO:QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0,
                              store_covariance=False, tol=0.0001)
2025-06-05 20:47:43,151:INFO:create_model() successfully completed......................................
2025-06-05 20:47:43,237:INFO:Initializing create_model()
2025-06-05 20:47:43,237:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000015F18134700>, estimator=LinearDiscriminantAnalysis(covariance_estimator=None, n_components=None,
                           priors=None, shrinkage=None, solver='svd',
                           store_covariance=False, tol=0.0001), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-06-05 20:47:43,237:INFO:Checking exceptions
2025-06-05 20:47:43,237:INFO:Importing libraries
2025-06-05 20:47:43,237:INFO:Copying training dataset
2025-06-05 20:47:43,249:INFO:Defining folds
2025-06-05 20:47:43,249:INFO:Declaring metric variables
2025-06-05 20:47:43,249:INFO:Importing untrained model
2025-06-05 20:47:43,249:INFO:Declaring custom model
2025-06-05 20:47:43,249:INFO:Linear Discriminant Analysis Imported successfully
2025-06-05 20:47:43,249:INFO:Cross validation set to False
2025-06-05 20:47:43,249:INFO:Fitting Model
2025-06-05 20:47:43,258:INFO:LinearDiscriminantAnalysis(covariance_estimator=None, n_components=None,
                           priors=None, shrinkage=None, solver='svd',
                           store_covariance=False, tol=0.0001)
2025-06-05 20:47:43,258:INFO:create_model() successfully completed......................................
2025-06-05 20:47:43,352:INFO:Initializing create_model()
2025-06-05 20:47:43,352:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000015F18134700>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-06-05 20:47:43,352:INFO:Checking exceptions
2025-06-05 20:47:43,352:INFO:Importing libraries
2025-06-05 20:47:43,352:INFO:Copying training dataset
2025-06-05 20:47:43,359:INFO:Defining folds
2025-06-05 20:47:43,359:INFO:Declaring metric variables
2025-06-05 20:47:43,359:INFO:Importing untrained model
2025-06-05 20:47:43,359:INFO:Declaring custom model
2025-06-05 20:47:43,359:INFO:Light Gradient Boosting Machine Imported successfully
2025-06-05 20:47:43,359:INFO:Cross validation set to False
2025-06-05 20:47:43,359:INFO:Fitting Model
2025-06-05 20:47:43,369:INFO:[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines
2025-06-05 20:47:43,369:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0,000041 seconds.
2025-06-05 20:47:43,369:INFO:You can set `force_row_wise=true` to remove the overhead.
2025-06-05 20:47:43,369:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2025-06-05 20:47:43,369:INFO:[LightGBM] [Info] Total Bins 82
2025-06-05 20:47:43,378:INFO:[LightGBM] [Info] Number of data points in the train set: 105, number of used features: 4
2025-06-05 20:47:43,378:INFO:[LightGBM] [Info] Start training from score -1,098612
2025-06-05 20:47:43,378:INFO:[LightGBM] [Info] Start training from score -1,098612
2025-06-05 20:47:43,378:INFO:[LightGBM] [Info] Start training from score -1,098612
2025-06-05 20:47:43,378:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:47:43,378:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:47:43,378:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:47:43,378:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:47:43,378:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:47:43,378:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:47:43,380:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:47:43,380:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:47:43,381:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:47:43,381:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:47:43,381:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:47:43,381:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:47:43,381:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:47:43,381:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:47:43,381:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:47:43,384:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:47:43,384:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:47:43,384:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:47:43,384:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:47:43,384:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:47:43,384:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:47:43,384:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:47:43,384:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:47:43,384:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:47:43,384:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:47:43,386:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:47:43,386:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:47:43,386:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:47:43,386:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:47:43,386:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:47:43,386:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:47:43,386:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:47:43,386:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:47:43,386:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:47:43,386:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:47:43,386:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:47:43,386:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:47:43,386:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:47:43,388:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:47:43,388:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:47:43,388:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:47:43,388:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:47:43,388:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:47:43,388:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:47:43,388:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:47:43,388:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:47:43,388:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:47:43,388:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:47:43,388:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:47:43,388:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:47:43,388:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:47:43,390:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:47:43,390:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:47:43,390:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:47:43,390:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:47:43,390:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:47:43,390:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:47:43,390:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:47:43,390:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:47:43,390:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:47:43,390:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:47:43,390:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:47:43,390:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:47:43,390:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:47:43,390:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:47:43,390:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:47:43,392:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:47:43,392:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:47:43,392:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:47:43,392:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:47:43,392:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:47:43,392:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:47:43,392:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:47:43,392:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:47:43,392:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:47:43,392:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:47:43,392:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:47:43,392:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:47:43,392:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:47:43,394:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:47:43,394:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:47:43,394:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:47:43,394:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:47:43,394:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:47:43,394:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:47:43,394:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:47:43,394:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:47:43,394:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:47:43,394:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:47:43,396:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:47:43,396:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:47:43,396:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:47:43,396:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:47:43,397:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:47:43,397:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:47:43,398:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:47:43,398:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:47:43,398:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:47:43,398:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:47:43,398:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:47:43,398:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:47:43,400:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:47:43,400:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:47:43,400:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:47:43,400:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:47:43,400:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:47:43,400:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:47:43,402:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:47:43,402:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:47:43,402:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:47:43,402:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:47:43,404:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:47:43,404:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:47:43,404:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:47:43,404:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:47:43,404:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:47:43,404:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:47:43,406:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:47:43,406:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:47:43,406:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:47:43,406:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:47:43,406:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:47:43,406:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:47:43,406:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:47:43,406:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:47:43,408:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:47:43,408:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:47:43,408:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:47:43,408:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:47:43,408:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:47:43,408:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:47:43,408:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:47:43,410:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:47:43,410:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:47:43,410:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:47:43,410:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:47:43,410:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:47:43,410:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:47:43,410:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:47:43,410:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:47:43,410:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:47:43,410:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:47:43,410:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:47:43,412:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:47:43,412:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:47:43,412:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:47:43,412:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:47:43,415:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:47:43,415:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:47:43,416:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:47:43,416:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:47:43,418:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:47:43,418:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:47:43,418:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:47:43,418:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:47:43,418:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:47:43,420:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:47:43,420:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:47:43,420:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:47:43,420:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:47:43,420:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:47:43,420:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:47:43,420:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:47:43,420:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:47:43,422:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:47:43,422:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:47:43,422:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:47:43,422:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:47:43,422:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:47:43,422:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:47:43,422:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:47:43,422:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:47:43,422:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:47:43,422:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:47:43,424:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:47:43,424:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:47:43,424:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:47:43,424:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:47:43,424:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:47:43,424:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:47:43,424:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:47:43,424:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:47:43,424:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:47:43,424:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:47:43,424:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:47:43,424:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:47:43,426:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:47:43,426:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:47:43,426:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:47:43,426:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:47:43,426:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:47:43,426:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:47:43,426:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:47:43,426:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:47:43,426:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:47:43,426:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:47:43,428:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:47:43,428:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:47:43,428:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:47:43,428:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:47:43,428:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:47:43,428:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:47:43,429:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:47:43,429:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:47:43,430:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:47:43,430:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:47:43,430:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:47:43,431:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:47:43,431:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:47:43,431:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:47:43,431:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:47:43,431:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:47:43,431:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:47:43,431:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:47:43,431:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:47:43,431:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:47:43,431:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:47:43,434:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:47:43,434:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:47:43,434:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:47:43,434:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:47:43,434:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:47:43,434:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:47:43,434:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:47:43,434:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:47:43,434:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:47:43,436:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:47:43,436:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:47:43,436:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:47:43,436:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:47:43,436:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:47:43,436:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:47:43,436:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:47:43,436:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:47:43,436:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:47:43,436:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:47:43,438:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:47:43,438:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:47:43,438:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:47:43,438:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:47:43,438:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:47:43,438:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:47:43,438:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:47:43,438:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:47:43,439:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:47:43,439:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:47:43,439:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:47:43,439:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:47:43,440:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:47:43,440:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:47:43,440:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:47:43,440:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:47:43,440:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:47:43,440:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:47:43,440:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:47:43,440:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:47:43,440:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:47:43,440:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:47:43,442:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:47:43,442:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:47:43,442:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:47:43,442:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:47:43,442:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:47:43,442:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:47:43,442:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:47:43,442:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:47:43,442:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:47:43,442:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:47:43,442:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:47:43,444:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:47:43,444:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:47:43,444:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:47:43,444:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:47:43,444:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:47:43,444:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:47:43,444:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:47:43,444:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:47:43,444:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:47:43,447:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:47:43,447:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:47:43,447:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:47:43,448:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:47:43,448:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:47:43,448:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:47:43,448:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:47:43,448:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:47:43,448:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:47:43,448:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:47:43,448:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:47:43,451:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:47:43,451:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:47:43,451:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:47:43,452:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:47:43,452:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:47:43,452:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:47:43,452:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:47:43,452:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:47:43,452:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:47:43,452:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:47:43,452:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:47:43,464:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2025-06-05 20:47:43,464:INFO:create_model() successfully completed......................................
2025-06-05 20:47:43,586:INFO:_master_model_container: 14
2025-06-05 20:47:43,586:INFO:_display_container: 2
2025-06-05 20:47:43,587:INFO:[LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=123, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False), KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
                     metric_params=None, n_jobs=-1, n_neighbors=5, p=2,
                     weights='uniform'), QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0,
                              store_covariance=False, tol=0.0001), LinearDiscriminantAnalysis(covariance_estimator=None, n_components=None,
                           priors=None, shrinkage=None, solver='svd',
                           store_covariance=False, tol=0.0001), LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)]
2025-06-05 20:47:43,588:INFO:compare_models() successfully completed......................................
2025-06-05 20:48:36,446:INFO:Initializing create_model()
2025-06-05 20:48:36,446:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000015F18134700>, estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=123, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False), fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-06-05 20:48:36,446:INFO:Checking exceptions
2025-06-05 20:48:36,470:INFO:Importing libraries
2025-06-05 20:48:36,470:INFO:Copying training dataset
2025-06-05 20:48:36,475:INFO:Defining folds
2025-06-05 20:48:36,476:INFO:Declaring metric variables
2025-06-05 20:48:36,484:INFO:Importing untrained model
2025-06-05 20:48:36,484:INFO:Declaring custom model
2025-06-05 20:48:36,493:INFO:Logistic Regression Imported successfully
2025-06-05 20:48:36,508:INFO:Starting cross validation
2025-06-05 20:48:36,509:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-06-05 20:48:36,575:WARNING:d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-05 20:48:36,578:WARNING:d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-05 20:48:36,582:WARNING:d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-05 20:48:36,582:WARNING:d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-05 20:48:36,589:WARNING:d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-05 20:48:36,590:WARNING:d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-05 20:48:36,597:WARNING:d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-05 20:48:36,602:WARNING:d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-05 20:48:36,624:INFO:Calculating mean and std
2025-06-05 20:48:36,625:INFO:Creating metrics dataframe
2025-06-05 20:48:36,631:INFO:Finalizing model
2025-06-05 20:48:36,656:INFO:Uploading results into container
2025-06-05 20:48:36,657:INFO:Uploading model into container now
2025-06-05 20:48:36,663:INFO:_master_model_container: 15
2025-06-05 20:48:36,663:INFO:_display_container: 3
2025-06-05 20:48:36,663:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=123, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2025-06-05 20:48:36,663:INFO:create_model() successfully completed......................................
2025-06-05 20:48:36,748:INFO:Initializing create_model()
2025-06-05 20:48:36,748:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000015F18134700>, estimator=KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
                     metric_params=None, n_jobs=-1, n_neighbors=5, p=2,
                     weights='uniform'), fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-06-05 20:48:36,748:INFO:Checking exceptions
2025-06-05 20:48:36,774:INFO:Importing libraries
2025-06-05 20:48:36,775:INFO:Copying training dataset
2025-06-05 20:48:36,778:INFO:Defining folds
2025-06-05 20:48:36,778:INFO:Declaring metric variables
2025-06-05 20:48:36,787:INFO:Importing untrained model
2025-06-05 20:48:36,787:INFO:Declaring custom model
2025-06-05 20:48:36,794:INFO:K Neighbors Classifier Imported successfully
2025-06-05 20:48:36,807:INFO:Starting cross validation
2025-06-05 20:48:36,808:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-06-05 20:48:36,975:INFO:Calculating mean and std
2025-06-05 20:48:36,976:INFO:Creating metrics dataframe
2025-06-05 20:48:36,985:INFO:Finalizing model
2025-06-05 20:48:37,001:INFO:Uploading results into container
2025-06-05 20:48:37,001:INFO:Uploading model into container now
2025-06-05 20:48:37,013:INFO:_master_model_container: 16
2025-06-05 20:48:37,014:INFO:_display_container: 4
2025-06-05 20:48:37,014:INFO:KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
                     metric_params=None, n_jobs=-1, n_neighbors=5, p=2,
                     weights='uniform')
2025-06-05 20:48:37,014:INFO:create_model() successfully completed......................................
2025-06-05 20:48:37,107:INFO:Initializing create_model()
2025-06-05 20:48:37,108:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000015F18134700>, estimator=QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0,
                              store_covariance=False, tol=0.0001), fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-06-05 20:48:37,108:INFO:Checking exceptions
2025-06-05 20:48:37,128:INFO:Importing libraries
2025-06-05 20:48:37,128:INFO:Copying training dataset
2025-06-05 20:48:37,131:INFO:Defining folds
2025-06-05 20:48:37,131:INFO:Declaring metric variables
2025-06-05 20:48:37,142:INFO:Importing untrained model
2025-06-05 20:48:37,142:INFO:Declaring custom model
2025-06-05 20:48:37,152:INFO:Quadratic Discriminant Analysis Imported successfully
2025-06-05 20:48:37,171:INFO:Starting cross validation
2025-06-05 20:48:37,172:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-06-05 20:48:37,217:WARNING:d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-05 20:48:37,218:WARNING:d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-05 20:48:37,218:WARNING:d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-05 20:48:37,223:WARNING:d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-05 20:48:37,225:WARNING:d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-05 20:48:37,226:WARNING:d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-05 20:48:37,229:WARNING:d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-05 20:48:37,230:WARNING:d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-05 20:48:37,230:WARNING:d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-05 20:48:37,239:WARNING:d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-05 20:48:37,261:INFO:Calculating mean and std
2025-06-05 20:48:37,261:INFO:Creating metrics dataframe
2025-06-05 20:48:37,268:INFO:Finalizing model
2025-06-05 20:48:37,286:INFO:Uploading results into container
2025-06-05 20:48:37,287:INFO:Uploading model into container now
2025-06-05 20:48:37,299:INFO:_master_model_container: 17
2025-06-05 20:48:37,299:INFO:_display_container: 5
2025-06-05 20:48:37,299:INFO:QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0,
                              store_covariance=False, tol=0.0001)
2025-06-05 20:48:37,300:INFO:create_model() successfully completed......................................
2025-06-05 20:48:37,401:INFO:Initializing create_model()
2025-06-05 20:48:37,401:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000015F18134700>, estimator=LinearDiscriminantAnalysis(covariance_estimator=None, n_components=None,
                           priors=None, shrinkage=None, solver='svd',
                           store_covariance=False, tol=0.0001), fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-06-05 20:48:37,401:INFO:Checking exceptions
2025-06-05 20:48:37,420:INFO:Importing libraries
2025-06-05 20:48:37,421:INFO:Copying training dataset
2025-06-05 20:48:37,424:INFO:Defining folds
2025-06-05 20:48:37,425:INFO:Declaring metric variables
2025-06-05 20:48:37,432:INFO:Importing untrained model
2025-06-05 20:48:37,432:INFO:Declaring custom model
2025-06-05 20:48:37,439:INFO:Linear Discriminant Analysis Imported successfully
2025-06-05 20:48:37,453:INFO:Starting cross validation
2025-06-05 20:48:37,454:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-06-05 20:48:37,496:WARNING:d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-05 20:48:37,496:WARNING:d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-05 20:48:37,497:WARNING:d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-05 20:48:37,499:WARNING:d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-05 20:48:37,502:WARNING:d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-05 20:48:37,503:WARNING:d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-05 20:48:37,505:WARNING:d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-05 20:48:37,506:WARNING:d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-05 20:48:37,506:WARNING:d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-05 20:48:37,514:WARNING:d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-05 20:48:37,529:INFO:Calculating mean and std
2025-06-05 20:48:37,529:INFO:Creating metrics dataframe
2025-06-05 20:48:37,537:INFO:Finalizing model
2025-06-05 20:48:37,550:INFO:Uploading results into container
2025-06-05 20:48:37,551:INFO:Uploading model into container now
2025-06-05 20:48:37,561:INFO:_master_model_container: 18
2025-06-05 20:48:37,562:INFO:_display_container: 6
2025-06-05 20:48:37,562:INFO:LinearDiscriminantAnalysis(covariance_estimator=None, n_components=None,
                           priors=None, shrinkage=None, solver='svd',
                           store_covariance=False, tol=0.0001)
2025-06-05 20:48:37,562:INFO:create_model() successfully completed......................................
2025-06-05 20:48:37,655:INFO:Initializing create_model()
2025-06-05 20:48:37,656:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000015F18134700>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-06-05 20:48:37,656:INFO:Checking exceptions
2025-06-05 20:48:37,667:INFO:Importing libraries
2025-06-05 20:48:37,667:INFO:Copying training dataset
2025-06-05 20:48:37,677:INFO:Defining folds
2025-06-05 20:48:37,677:INFO:Declaring metric variables
2025-06-05 20:48:37,683:INFO:Importing untrained model
2025-06-05 20:48:37,684:INFO:Declaring custom model
2025-06-05 20:48:37,691:INFO:Light Gradient Boosting Machine Imported successfully
2025-06-05 20:48:37,705:INFO:Starting cross validation
2025-06-05 20:48:37,705:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-06-05 20:48:38,276:INFO:Calculating mean and std
2025-06-05 20:48:38,277:INFO:Creating metrics dataframe
2025-06-05 20:48:38,286:INFO:Finalizing model
2025-06-05 20:48:38,300:INFO:[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines
2025-06-05 20:48:38,300:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000075 seconds.
2025-06-05 20:48:38,301:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-06-05 20:48:38,301:INFO:[LightGBM] [Info] Total Bins 82
2025-06-05 20:48:38,301:INFO:[LightGBM] [Info] Number of data points in the train set: 105, number of used features: 4
2025-06-05 20:48:38,301:INFO:[LightGBM] [Info] Start training from score -1,098612
2025-06-05 20:48:38,301:INFO:[LightGBM] [Info] Start training from score -1,098612
2025-06-05 20:48:38,301:INFO:[LightGBM] [Info] Start training from score -1,098612
2025-06-05 20:48:38,301:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:48:38,302:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:48:38,302:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:48:38,302:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:48:38,302:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:48:38,303:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:48:38,303:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:48:38,303:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:48:38,303:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:48:38,304:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:48:38,304:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:48:38,304:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:48:38,304:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:48:38,305:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:48:38,305:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:48:38,305:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:48:38,305:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:48:38,306:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:48:38,306:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:48:38,306:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:48:38,306:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:48:38,307:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:48:38,307:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:48:38,307:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:48:38,308:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:48:38,308:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:48:38,308:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:48:38,308:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:48:38,309:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:48:38,309:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:48:38,309:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:48:38,310:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:48:38,310:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:48:38,311:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:48:38,311:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:48:38,311:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:48:38,311:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:48:38,312:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:48:38,312:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:48:38,312:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:48:38,313:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:48:38,313:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:48:38,313:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:48:38,313:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:48:38,314:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:48:38,314:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:48:38,314:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:48:38,314:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:48:38,315:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:48:38,315:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:48:38,315:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:48:38,315:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:48:38,316:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:48:38,316:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:48:38,316:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:48:38,316:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:48:38,317:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:48:38,317:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:48:38,317:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:48:38,317:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:48:38,317:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:48:38,318:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:48:38,318:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:48:38,319:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:48:38,319:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:48:38,319:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:48:38,319:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:48:38,319:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:48:38,320:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:48:38,320:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:48:38,320:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:48:38,320:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:48:38,321:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:48:38,321:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:48:38,321:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:48:38,321:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:48:38,322:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:48:38,322:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:48:38,322:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:48:38,323:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:48:38,323:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:48:38,323:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:48:38,324:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:48:38,324:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:48:38,324:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:48:38,325:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:48:38,325:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:48:38,325:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:48:38,326:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:48:38,326:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:48:38,326:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:48:38,327:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:48:38,327:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:48:38,328:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:48:38,328:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:48:38,328:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:48:38,328:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:48:38,329:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:48:38,329:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:48:38,330:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:48:38,331:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:48:38,331:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:48:38,332:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:48:38,332:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:48:38,333:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:48:38,334:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:48:38,334:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:48:38,334:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:48:38,335:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:48:38,335:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:48:38,335:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:48:38,335:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:48:38,336:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:48:38,336:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:48:38,336:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:48:38,337:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:48:38,337:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:48:38,337:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:48:38,337:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:48:38,338:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:48:38,339:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:48:38,339:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:48:38,342:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:48:38,342:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:48:38,342:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:48:38,342:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:48:38,343:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:48:38,343:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:48:38,343:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:48:38,344:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:48:38,344:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:48:38,344:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:48:38,345:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:48:38,345:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:48:38,345:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:48:38,346:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:48:38,346:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:48:38,346:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:48:38,346:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:48:38,347:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:48:38,347:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:48:38,347:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:48:38,347:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:48:38,348:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:48:38,348:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:48:38,348:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:48:38,348:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:48:38,349:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:48:38,349:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:48:38,349:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:48:38,350:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:48:38,350:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:48:38,350:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:48:38,350:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:48:38,351:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:48:38,351:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:48:38,351:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:48:38,351:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:48:38,352:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:48:38,352:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:48:38,352:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:48:38,352:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:48:38,354:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:48:38,362:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:48:38,370:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:48:38,371:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:48:38,371:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:48:38,372:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:48:38,372:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:48:38,372:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:48:38,372:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:48:38,373:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:48:38,373:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:48:38,374:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:48:38,374:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:48:38,374:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:48:38,374:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:48:38,375:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:48:38,375:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:48:38,375:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:48:38,376:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:48:38,376:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:48:38,377:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:48:38,378:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:48:38,378:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:48:38,379:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:48:38,380:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:48:38,380:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:48:38,381:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:48:38,381:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:48:38,382:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:48:38,382:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:48:38,383:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:48:38,384:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:48:38,384:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:48:38,385:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:48:38,385:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:48:38,385:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:48:38,386:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:48:38,386:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:48:38,387:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:48:38,387:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:48:38,388:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:48:38,388:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:48:38,388:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:48:38,389:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:48:38,389:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:48:38,389:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:48:38,390:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:48:38,390:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:48:38,391:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:48:38,391:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:48:38,391:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:48:38,391:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:48:38,392:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:48:38,392:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:48:38,392:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:48:38,392:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:48:38,393:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:48:38,393:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:48:38,393:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:48:38,394:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:48:38,394:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:48:38,395:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:48:38,395:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:48:38,395:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:48:38,396:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:48:38,396:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:48:38,396:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:48:38,396:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:48:38,397:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:48:38,397:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:48:38,397:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:48:38,397:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:48:38,397:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:48:38,398:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:48:38,398:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:48:38,398:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:48:38,398:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:48:38,399:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:48:38,399:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:48:38,399:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:48:38,399:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:48:38,399:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:48:38,399:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:48:38,400:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:48:38,400:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:48:38,400:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:48:38,400:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:48:38,400:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:48:38,400:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:48:38,401:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:48:38,401:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:48:38,401:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:48:38,401:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:48:38,401:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:48:38,401:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:48:38,401:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:48:38,402:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:48:38,402:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:48:38,402:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:48:38,402:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:48:38,402:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:48:38,402:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:48:38,402:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:48:38,403:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:48:38,403:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:48:38,403:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:48:38,403:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:48:38,403:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:48:38,403:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:48:38,403:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:48:38,404:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:48:38,404:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:48:38,404:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:48:38,404:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:48:38,404:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:48:38,404:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:48:38,404:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:48:38,405:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:48:38,405:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:48:38,405:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:48:38,405:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:48:38,405:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:48:38,405:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:48:38,406:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:48:38,406:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:48:38,406:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:48:38,406:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:48:38,406:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:48:38,407:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:48:38,407:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:48:38,407:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:48:38,408:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:48:38,408:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:48:38,408:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:48:38,408:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:48:38,408:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:48:38,408:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:48:38,409:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-06-05 20:48:38,423:INFO:Uploading results into container
2025-06-05 20:48:38,424:INFO:Uploading model into container now
2025-06-05 20:48:38,437:INFO:_master_model_container: 19
2025-06-05 20:48:38,437:INFO:_display_container: 7
2025-06-05 20:48:38,438:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2025-06-05 20:48:38,439:INFO:create_model() successfully completed......................................
2025-06-05 20:48:55,712:INFO:Initializing evaluate_model()
2025-06-05 20:48:55,713:INFO:evaluate_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000015F18134700>, estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=123, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False), fold=None, fit_kwargs=None, plot_kwargs=None, feature_name=None, groups=None)
2025-06-05 20:48:55,722:INFO:Initializing plot_model()
2025-06-05 20:48:55,723:INFO:plot_model(plot=pipeline, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), verbose=False, display=None, display_format=None, estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=123, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False), feature_name=None, fit_kwargs={}, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000015F18134700>, system=True)
2025-06-05 20:48:55,723:INFO:Checking exceptions
2025-06-05 20:48:55,725:INFO:Preloading libraries
2025-06-05 20:48:55,725:INFO:Copying training dataset
2025-06-05 20:48:55,725:INFO:Plot type: pipeline
2025-06-05 20:48:55,877:INFO:Visual Rendered Successfully
2025-06-05 20:48:55,947:INFO:plot_model() successfully completed......................................
2025-06-05 20:49:03,259:INFO:Initializing plot_model()
2025-06-05 20:49:03,259:INFO:plot_model(plot=ks, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), verbose=False, display=None, display_format=None, estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=123, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False), feature_name=None, fit_kwargs={}, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000015F18134700>, system=True)
2025-06-05 20:49:03,259:INFO:Checking exceptions
2025-06-05 20:49:03,261:INFO:Preloading libraries
2025-06-05 20:49:03,261:INFO:Copying training dataset
2025-06-05 20:49:03,261:INFO:Plot type: ks
2025-06-05 20:49:03,261:INFO:Generating predictions / predict_proba on X_test
2025-06-05 20:49:05,233:INFO:Initializing plot_model()
2025-06-05 20:49:05,233:INFO:plot_model(plot=tree, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), verbose=False, display=None, display_format=None, estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=123, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False), feature_name=None, fit_kwargs={}, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000015F18134700>, system=True)
2025-06-05 20:49:05,233:INFO:Checking exceptions
2025-06-05 20:49:06,642:INFO:Initializing plot_model()
2025-06-05 20:49:06,642:INFO:plot_model(plot=gain, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), verbose=False, display=None, display_format=None, estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=123, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False), feature_name=None, fit_kwargs={}, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000015F18134700>, system=True)
2025-06-05 20:49:06,642:INFO:Checking exceptions
2025-06-05 20:49:06,645:INFO:Preloading libraries
2025-06-05 20:49:06,645:INFO:Copying training dataset
2025-06-05 20:49:06,645:INFO:Plot type: gain
2025-06-05 20:49:06,646:INFO:Generating predictions / predict_proba on X_test
2025-06-05 20:49:07,434:INFO:Initializing plot_model()
2025-06-05 20:49:07,434:INFO:plot_model(plot=auc, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), verbose=False, display=None, display_format=None, estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=123, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False), feature_name=None, fit_kwargs={}, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000015F18134700>, system=True)
2025-06-05 20:49:07,434:INFO:Checking exceptions
2025-06-05 20:49:07,436:INFO:Preloading libraries
2025-06-05 20:49:07,436:INFO:Copying training dataset
2025-06-05 20:49:07,437:INFO:Plot type: auc
2025-06-05 20:49:07,486:INFO:Fitting Model
2025-06-05 20:49:07,486:WARNING:d:\Projects\clase_ia\AA_CLASE4\.venv\lib\site-packages\sklearn\base.py:493: UserWarning: X does not have valid feature names, but LogisticRegression was fitted with feature names
  warnings.warn(

2025-06-05 20:49:07,486:INFO:Scoring test/hold-out set
2025-06-05 20:49:07,655:INFO:Visual Rendered Successfully
2025-06-05 20:49:07,794:INFO:plot_model() successfully completed......................................
2025-06-05 20:49:09,724:INFO:Initializing plot_model()
2025-06-05 20:49:09,725:INFO:plot_model(plot=pipeline, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), verbose=False, display=None, display_format=None, estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=123, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False), feature_name=None, fit_kwargs={}, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000015F18134700>, system=True)
2025-06-05 20:49:09,725:INFO:Checking exceptions
2025-06-05 20:49:09,727:INFO:Preloading libraries
2025-06-05 20:49:09,727:INFO:Copying training dataset
2025-06-05 20:49:09,727:INFO:Plot type: pipeline
2025-06-05 20:49:09,805:INFO:Visual Rendered Successfully
2025-06-05 20:49:09,886:INFO:plot_model() successfully completed......................................
2025-06-05 20:50:52,606:INFO:Initializing save_model()
2025-06-05 20:50:52,606:INFO:save_model(model=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=123, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False), model_name=best_iris_classifier, prep_pipe_=Pipeline(memory=FastMemory(location=C:\Users\ORPHEUX\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['sepal length (cm)',
                                             'sepal width (cm)',
                                             'petal length (cm)',
                                             'petal width (cm)'],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='mea...
                ('categorical_imputer',
                 TransformerWrapper(exclude=None, include=[],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='most_frequent'))),
                ('clean_column_names',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=CleanColumnNames(match='[\\]\\[\\,\\{\\}\\"\\:]+')))],
         verbose=False), verbose=True, use_case=MLUsecase.CLASSIFICATION, kwargs={})
2025-06-05 20:50:52,606:INFO:Adding model into prep_pipe
2025-06-05 20:50:52,606:INFO:best_iris_classifier.pkl saved in current working directory
2025-06-05 20:50:52,624:INFO:Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['sepal length (cm)',
                                             'sepal width (cm)',
                                             'petal length (cm)',
                                             'petal width (cm)'],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='mean'))),
                ('categorical_imputer',
                 TransformerWr...
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=CleanColumnNames(match='[\\]\\[\\,\\{\\}\\"\\:]+'))),
                ('trained_model',
                 LogisticRegression(C=1.0, class_weight=None, dual=False,
                                    fit_intercept=True, intercept_scaling=1,
                                    l1_ratio=None, max_iter=1000,
                                    multi_class='auto', n_jobs=None,
                                    penalty='l2', random_state=123,
                                    solver='lbfgs', tol=0.0001, verbose=0,
                                    warm_start=False))],
         verbose=False)
2025-06-05 20:50:52,624:INFO:save_model() successfully completed......................................
2025-06-05 20:51:03,566:INFO:Initializing load_model()
2025-06-05 20:51:03,566:INFO:load_model(model_name=best_iris_classifier, platform=None, authentication=None, verbose=True)
