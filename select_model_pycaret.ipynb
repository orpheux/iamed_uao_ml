{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70ee5cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# PyCaret clustering imports\n",
    "from pycaret.clustering import *\n",
    "\n",
    "# Imports adicionales para an√°lisis\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import silhouette_score, calinski_harabasz_score\n",
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "import time\n",
    "import logging\n",
    "\n",
    "# Configuraci√≥n de logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler('clustering_medicamentos.log'),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "print(\"üî¨ SISTEMA DE HOMOLOGACI√ìN AUTOM√ÅTICA DE MEDICAMENTOS\")\n",
    "print(\"=\"*70)\n",
    "print(\"üìä Objetivo: Clustering para recomendaci√≥n de medicamentos similares\")\n",
    "print(\"üéØ Estrategia: Entrenar con todo, recomendar solo v√°lidos\")\n",
    "print(\"üîù Output: Top 5 medicamentos similares por CUM inv√°lido\")\n",
    "print(\"=\"*70)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f84457b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(\"Cargando datasets...\")\n",
    "\n",
    "# Rutas de los archivos\n",
    "path_original = Path(\"./data/medicamentos_train_original.parquet\")\n",
    "path_encoded = Path(\"./data/medicamentos_train_preprocesados.parquet\")\n",
    "\n",
    "# Verificar que existen los archivos\n",
    "if not path_original.exists():\n",
    "    raise FileNotFoundError(f\"No se encontr√≥: {path_original}\")\n",
    "if not path_encoded.exists():\n",
    "    raise FileNotFoundError(f\"No se encontr√≥: {path_encoded}\")\n",
    "\n",
    "# Cargar datasets\n",
    "df_original = pd.read_parquet(path_original)\n",
    "df_encoded = pd.read_parquet(path_encoded)\n",
    "\n",
    "print(f\"üìÅ Dataset Original cargado: {df_original.shape}\")\n",
    "print(f\"üìÅ Dataset Encodificado cargado: {df_encoded.shape}\")\n",
    "print()\n",
    "\n",
    "# Informaci√≥n b√°sica de los datasets\n",
    "logger.info(\"Informaci√≥n de datasets:\")\n",
    "logger.info(f\"Original - Shape: {df_original.shape}, Memory: {df_original.memory_usage(deep=True).sum() / 1024**2:.1f} MB\")\n",
    "logger.info(f\"Encodificado - Shape: {df_encoded.shape}, Memory: {df_encoded.memory_usage(deep=True).sum() / 1024**2:.1f} MB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6555480",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# PASO 3: AN√ÅLISIS EXPLORATORIO R√ÅPIDO\n",
    "# =============================================================================\n",
    "\n",
    "print(\"üîç AN√ÅLISIS EXPLORATORIO DE AMBOS DATASETS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Dataset Original\n",
    "print(\"üìã DATASET ORIGINAL:\")\n",
    "print(f\"   - Columnas: {list(df_original.columns)}\")\n",
    "print(f\"   - Tipos de datos: {df_original.dtypes.value_counts().to_dict()}\")\n",
    "print(f\"   - Valores nulos: {df_original.isnull().sum().sum()}\")\n",
    "print()\n",
    "\n",
    "# Dataset Encodificado  \n",
    "print(\"üìã DATASET ENCODIFICADO:\")\n",
    "print(f\"   - Columnas: {df_encoded.shape[1]} columnas\")\n",
    "print(f\"   - Tipos de datos: {df_encoded.dtypes.value_counts().to_dict()}\")\n",
    "print(f\"   - Valores nulos: {df_encoded.isnull().sum().sum()}\")\n",
    "print()\n",
    "\n",
    "# Verificar columnas de validez en ambos datasets\n",
    "validez_cols = ['ESTADO REGISTRO', 'ESTADO CUM', 'MUESTRA M√âDICA']\n",
    "print(\"üîç VERIFICACI√ìN DE COLUMNAS DE VALIDEZ:\")\n",
    "\n",
    "# Inicializar variable para evitar errores\n",
    "validos_orig = 0\n",
    "\n",
    "# Verificar si todas las columnas est√°n presentes\n",
    "all_cols_present = all(col in df_original.columns for col in validez_cols)\n",
    "\n",
    "if all_cols_present:\n",
    "    validos_orig = len(df_original[\n",
    "        (df_original['ESTADO REGISTRO'] == 'Vigente') & \n",
    "        (df_original['ESTADO CUM'] == 'Activo') & \n",
    "        (df_original['MUESTRA M√âDICA'] == 'No')\n",
    "    ])\n",
    "\n",
    "for col in validez_cols:\n",
    "    if col in df_original.columns:\n",
    "        print(f\"   ‚úÖ {col} presente en dataset original\")\n",
    "    else:\n",
    "        print(f\"   ‚ùå {col} NO presente en dataset original\")\n",
    "\n",
    "print(f\"   üìä Medicamentos v√°lidos en original: {validos_orig:,}\")\n",
    "\n",
    "# Para dataset encodificado, buscar columnas de validez\n",
    "validez_encoded = []\n",
    "for col in df_encoded.columns:\n",
    "    if any(v.lower() in col.lower() for v in ['estado', 'muestra']):\n",
    "        validez_encoded.append(col)\n",
    "\n",
    "print(f\"   üìä Columnas de validez en encodificado: {validez_encoded}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a04cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# PASO 9: PREPARACI√ìN PARA CLUSTERING - DATASET ENCODIFICADO\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\nüéØ PREPARACI√ìN PARA CLUSTERING - DATASET ENCODIFICADO\")\n",
    "print(\"=\"*57)\n",
    "\n",
    "# El dataset encodificado ya debe tener las columnas de validez encodificadas\n",
    "print(f\"üìä Dataset encodificado shape: {df_encoded.shape}\")\n",
    "print(f\"üìã Tipos de datos: {df_encoded.dtypes.value_counts().to_dict()}\")\n",
    "\n",
    "# Para dataset encodificado, necesitamos identificar las columnas de validez\n",
    "# Asumiendo que ya fueron encodificadas, buscar patrones\n",
    "print(\"üîç Buscando columnas de validez en dataset encodificado...\")\n",
    "\n",
    "# Buscar columnas relacionadas con validez (pueden estar encodificadas)\n",
    "possible_validity_cols = [col for col in df_encoded.columns \n",
    "                         if any(term in col.lower() for term in ['estado', 'muestra', 'vigente', 'activo'])]\n",
    "\n",
    "print(f\"üìã Posibles columnas de validez encontradas: {possible_validity_cols}\")\n",
    "\n",
    "# Usar TODO el dataset encodificado - TU ya hiciste el trabajo de selecci√≥n\n",
    "df_clustering_encoded = df_encoded.copy()\n",
    "\n",
    "print(f\"üìä Columnas para clustering encodificado: {df_clustering_encoded.shape[1]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c38bba31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# PASO 10: CONFIGURACI√ìN DE PYCARET CLUSTERING - DATASET ENCODIFICADO\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\nüöÄ CONFIGURACI√ìN DE PYCARET CLUSTERING - DATASET ENCODIFICADO\")\n",
    "print(\"=\"*62)\n",
    "\n",
    "logger.info(\"Iniciando setup de PyCaret para dataset encodificado...\")\n",
    "\n",
    "try:\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Para dataset encodificado, setup simple como el ejemplo de clase\n",
    "    cluster_setup_encoded = setup(\n",
    "        data=df_clustering_encoded,\n",
    "        session_id=123,                   # Para reproducibilidad\n",
    "        normalize=False,                  # ‚ùå NO normalizar - ya est√° hecho\n",
    "        transformation=False,             # ‚ùå NO transformar - ya est√° hecho  \n",
    "        pca=False,                        # ‚ùå NO aplicar PCA - usar todas las features\n",
    "        remove_multicollinearity=False,   # ‚ùå NO remover multicolinealidad - ya se manej√≥\n",
    "        remove_outliers=False,            # ‚ùå NO remover outliers - conservar datos\n",
    "        preprocess=False,                 # ‚ùå NO hacer preprocessing adicional\n",
    "        use_gpu=True,                     # Usar GPU si est√° disponible\n",
    "        \n",
    "        \n",
    "    )\n",
    "    \n",
    "    setup_time_encoded = time.time() - start_time\n",
    "    logger.info(f\"Setup encodificado completado en {setup_time_encoded:.2f} segundos\")\n",
    "    print(f\"‚úÖ Setup PyCaret completado para dataset encodificado ({setup_time_encoded:.2f}s)\")\n",
    "    \n",
    "except Exception as e:\n",
    "    logger.error(f\"Error en setup encodificado: {str(e)}\")\n",
    "    print(f\"‚ùå Error en setup encodificado: {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f8c11b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# COMPARACI√ìN COMPLETA DE TODOS LOS ALGORITMOS DISPONIBLES\n",
    "# =============================================================================\n",
    "\n",
    "from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score\n",
    "import time\n",
    "\n",
    "#  algoritmos de PyCaret\n",
    "algoritmos_clustering = [\n",
    "    # si 'kmeans',      # K-Means Clustering\n",
    "    #'ap',          # Affinity Propagation\n",
    "    #'meanshift',   # Mean shift Clustering\n",
    "    # 'sc',          # Spectral Clustering\n",
    "    # 'hclust',      # Agglomerative Clustering\n",
    "    'dbscan',      # Density-Based Spatial Clustering\n",
    "    'optics',      # OPTICS Clustering\n",
    "    'birch',       # Birch Clustering\n",
    "    'kmodes'       # K-Modes Clustering\n",
    "]\n",
    "\n",
    "\n",
    "# Diccionario para almacenar resultados\n",
    "resultados_comparacion = {}\n",
    "metricas_comparacion = []\n",
    "\n",
    "print(\"üî¨ COMPARACI√ìN COMPLETA DE TODOS LOS ALGORITMOS DE CLUSTERING\")\n",
    "print(\"=\"*65)\n",
    "print(f\"üéØ Algoritmos a probar: {len(algoritmos_clustering)}\")\n",
    "for i, algo in enumerate(algoritmos_clustering, 1):\n",
    "    print(f\"   {i}. {algo.upper()}\")\n",
    "print(\"=\"*65)\n",
    "\n",
    "for i, algoritmo in enumerate(algoritmos_clustering, 1):\n",
    "    print(f\"\\nüß™ Probando {i}/{len(algoritmos_clustering)}: {algoritmo.upper()}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    try:\n",
    "        # Medir tiempo de entrenamiento\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Crear modelo con par√°metros espec√≠ficos seg√∫n algoritmo\n",
    "        if algoritmo == 'kmeans':\n",
    "            # K-means: especificar n√∫mero de clusters\n",
    "            modelo = create_model(algoritmo, num_clusters=20)\n",
    "        elif algoritmo == 'sc':\n",
    "            # Spectral: especificar n√∫mero de clusters\n",
    "            modelo = create_model(algoritmo, num_clusters=15)\n",
    "        elif algoritmo == 'hclust':\n",
    "            # Hierarchical: especificar n√∫mero de clusters\n",
    "            modelo = create_model(algoritmo, num_clusters=25)\n",
    "        elif algoritmo == 'birch':\n",
    "            # BIRCH: especificar n√∫mero de clusters\n",
    "            modelo = create_model(algoritmo, num_clusters=20)\n",
    "        elif algoritmo == 'kmodes':\n",
    "            # K-modes: para variables categ√≥ricas\n",
    "            modelo = create_model(algoritmo, num_clusters=15)\n",
    "        else:\n",
    "            # Para AP, MEANSHIFT, DBSCAN, OPTICS: autom√°tico\n",
    "            modelo = create_model(algoritmo)\n",
    "        \n",
    "        training_time = time.time() - start_time\n",
    "        \n",
    "        # Asignar clusters\n",
    "        print(f\"   ‚è≥ Asignando clusters...\")\n",
    "        assign_start = time.time()\n",
    "        resultados = assign_model(modelo)\n",
    "        assign_time = time.time() - assign_start\n",
    "        \n",
    "        # Verificar resultados\n",
    "        n_clusters = resultados['Cluster'].nunique()\n",
    "        n_noise = (resultados['Cluster'] == -1).sum() if -1 in resultados['Cluster'].values else 0\n",
    "        total_time = training_time + assign_time\n",
    "        \n",
    "        print(f\"   ‚úÖ Completado en {total_time:.2f}s (Train: {training_time:.2f}s, Assign: {assign_time:.2f}s)\")\n",
    "        print(f\"   üìä Clusters generados: {n_clusters}\")\n",
    "        print(f\"   üîç Puntos de ruido: {n_noise} ({n_noise/len(resultados)*100:.1f}%)\")\n",
    "        \n",
    "        # Calcular m√©tricas de evaluaci√≥n\n",
    "        if n_clusters > 1:\n",
    "            try:\n",
    "                print(f\"   üìà Calculando m√©tricas...\")\n",
    "                \n",
    "                # Obtener datos originales\n",
    "                X = get_config('X_train')\n",
    "                labels = resultados['Cluster'].values\n",
    "                \n",
    "                # Filtrar ruido para m√©tricas (si existe)\n",
    "                if n_noise > 0:\n",
    "                    mask_no_noise = labels != -1\n",
    "                    X_clean = X[mask_no_noise]\n",
    "                    labels_clean = labels[mask_no_noise]\n",
    "                    n_clusters_clean = len(np.unique(labels_clean))\n",
    "                else:\n",
    "                    X_clean = X\n",
    "                    labels_clean = labels\n",
    "                    n_clusters_clean = n_clusters\n",
    "                \n",
    "                # Solo calcular m√©tricas si hay suficientes clusters\n",
    "                if n_clusters_clean > 1:\n",
    "                    # silhouette = silhouette_score(X_clean, labels_clean)\n",
    "                    calinski = calinski_harabasz_score(X_clean, labels_clean)\n",
    "                    davies_bouldin = davies_bouldin_score(X_clean, labels_clean)\n",
    "                    \n",
    "                    # print(f\"   üéØ Silhouette Score: {silhouette:.4f}\")\n",
    "                    print(f\"   üéØ Calinski-Harabasz: {calinski:.2f}\")\n",
    "                    print(f\"   üéØ Davies-Bouldin: {davies_bouldin:.4f}\")\n",
    "                    \n",
    "                    # Guardar resultados\n",
    "                    resultados_comparacion[algoritmo] = {\n",
    "                        'modelo': modelo,\n",
    "                        'resultados': resultados,\n",
    "                        'tiempo_total': total_time,\n",
    "                        'tiempo_entrenamiento': training_time,\n",
    "                        'tiempo_asignacion': assign_time,\n",
    "                        'n_clusters': n_clusters,\n",
    "                        'n_clusters_clean': n_clusters_clean,\n",
    "                        'n_noise': n_noise,\n",
    "                        'pct_noise': n_noise/len(resultados)*100,\n",
    "                        # 'silhouette': silhouette,\n",
    "                        'calinski_harabasz': calinski,\n",
    "                        'davies_bouldin': davies_bouldin\n",
    "                    }\n",
    "                    \n",
    "                    # Para tabla comparativa\n",
    "                    metricas_comparacion.append({\n",
    "                        'Algoritmo': algoritmo.upper(),\n",
    "                        'Tiempo_Total': round(total_time, 2),\n",
    "                        'N_Clusters': n_clusters,\n",
    "                        'Ruido_%': round(n_noise/len(resultados)*100, 1),\n",
    "                        # 'Silhouette': round(silhouette, 4),\n",
    "                        'Calinski_H': round(calinski, 2),\n",
    "                        'Davies_B': round(davies_bouldin, 4),\n",
    "                        'Status': '‚úÖ'\n",
    "                    })\n",
    "                    \n",
    "                else:\n",
    "                    print(f\"   ‚ö†Ô∏è Insuficientes clusters limpios para m√©tricas\")\n",
    "                    metricas_comparacion.append({\n",
    "                        'Algoritmo': algoritmo.upper(),\n",
    "                        'Tiempo_Total': round(total_time, 2),\n",
    "                        'N_Clusters': n_clusters,\n",
    "                        'Ruido_%': round(n_noise/len(resultados)*100, 1),\n",
    "                        'Silhouette': 'N/A',\n",
    "                        'Calinski_H': 'N/A',\n",
    "                        'Davies_B': 'N/A',\n",
    "                        'Status': '‚ö†Ô∏è'\n",
    "                    })\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"   ‚ùå Error calculando m√©tricas: {str(e)}\")\n",
    "                metricas_comparacion.append({\n",
    "                    'Algoritmo': algoritmo.upper(),\n",
    "                    'Tiempo_Total': round(total_time, 2),\n",
    "                    'N_Clusters': n_clusters,\n",
    "                    'Ruido_%': round(n_noise/len(resultados)*100, 1) if n_noise > 0 else 0,\n",
    "                    'Silhouette': 'ERROR',\n",
    "                    'Calinski_H': 'ERROR',\n",
    "                    'Davies_B': 'ERROR',\n",
    "                    'Status': '‚ùå'\n",
    "                })\n",
    "                \n",
    "        else:\n",
    "            print(f\"   ‚ö†Ô∏è Solo 1 cluster generado - algoritmo no √∫til\")\n",
    "            metricas_comparacion.append({\n",
    "                'Algoritmo': algoritmo.upper(),\n",
    "                'Tiempo_Total': round(total_time, 2),\n",
    "                'N_Clusters': n_clusters,\n",
    "                'Ruido_%': 0,\n",
    "                'Silhouette': 'N/A',\n",
    "                'Calinski_H': 'N/A',\n",
    "                'Davies_B': 'N/A',\n",
    "                'Status': '‚ö†Ô∏è'\n",
    "            })\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå ERROR CR√çTICO con {algoritmo}: {str(e)}\")\n",
    "        metricas_comparacion.append({\n",
    "            'Algoritmo': algoritmo.upper(),\n",
    "            'Tiempo_Total': 'FALL√ì',\n",
    "            'N_Clusters': 'FALL√ì',\n",
    "            'Ruido_%': 'FALL√ì',\n",
    "            'Silhouette': 'FALL√ì',\n",
    "            'Calinski_H': 'FALL√ì',\n",
    "            'Davies_B': 'FALL√ì',\n",
    "            'Status': 'üí•'\n",
    "        })\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d5d4f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# TABLA COMPARATIVA FINAL\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"üìä TABLA COMPARATIVA COMPLETA DE TODOS LOS ALGORITMOS\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "if metricas_comparacion:\n",
    "    df_comparacion = pd.DataFrame(metricas_comparacion)\n",
    "    \n",
    "    # Filtrar solo algoritmos exitosos para ranking\n",
    "    df_exitosos = df_comparacion[df_comparacion['Status'] == '‚úÖ'].copy()\n",
    "    \n",
    "    if len(df_exitosos) > 0:\n",
    "        # Ordenar por Silhouette Score (mayor es mejor)\n",
    "        df_exitosos = df_exitosos.sort_values('Silhouette', ascending=False)\n",
    "        \n",
    "        print(\"üèÜ RANKING DE ALGORITMOS EXITOSOS:\")\n",
    "        print(\"-\" * 50)\n",
    "        print(df_exitosos.to_string(index=False))\n",
    "        \n",
    "        print(f\"\\nü•á PODIUM DE GANADORES:\")\n",
    "        print(\"-\" * 30)\n",
    "        for i, (idx, row) in enumerate(df_exitosos.head(3).iterrows(), 1):\n",
    "            emoji = \"ü•á\" if i == 1 else \"ü•à\" if i == 2 else \"ü•â\"\n",
    "            print(f\"{emoji} {i}¬∞ lugar: {row['Algoritmo']} (Silhouette: {row['Silhouette']})\")\n",
    "            \n",
    "        # Guardar resultados\n",
    "        df_comparacion.to_csv('./comparacion_completa_clustering.csv', index=False)\n",
    "        print(f\"\\nüíæ Resultados completos guardados en: comparacion_completa_clustering.csv\")\n",
    "        \n",
    "    else:\n",
    "        print(\"‚ùå Ning√∫n algoritmo fue completamente exitoso\")\n",
    "        \n",
    "    # Mostrar tabla completa (incluyendo errores)\n",
    "    print(f\"\\nüìã RESUMEN COMPLETO (incluye errores):\")\n",
    "    print(\"-\" * 40)\n",
    "    print(df_comparacion.to_string(index=False))\n",
    "    \n",
    "else:\n",
    "    print(\"üí• No se pudieron ejecutar algoritmos\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"üéâ COMPARACI√ìN COMPLETA DE 9 ALGORITMOS FINALIZADA\")\n",
    "print(\"=\"*100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
